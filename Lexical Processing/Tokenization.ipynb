{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlM4mGAkTv0+UC9MGOGuNZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohaHussain/Machine-Learning/blob/main/Lexical%20Processing/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word tokenization"
      ],
      "metadata": {
        "id": "iseXPzP1Uj5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnfF_H-9UY1_",
        "outputId": "4ee11a64-5e4c-421e-8756-98cbb3ef22a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token. The sensitive data still generally needs to be stored securely at one centralized location for subsequent reference and requires strong protections around it.\n"
          ]
        }
      ],
      "source": [
        "doc = \"Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token. The sensitive data still generally needs to be stored securely at one centralized location for subsequent reference and requires strong protections around it.\"\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing on spaces\n",
        "\n",
        "print(doc.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5N7DMGTU7ey",
        "outputId": "4c3d4eca-51d6-4956-8355-e2fe00db0147"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'refers', 'to', 'a', 'process', 'by', 'which', 'a', 'piece', 'of', 'sensitive', 'data,', 'such', 'as', 'a', 'credit', 'card', 'number,', 'is', 'replaced', 'by', 'a', 'surrogate', 'value', 'known', 'as', 'a', 'token.', 'The', 'sensitive', 'data', 'still', 'generally', 'needs', 'to', 'be', 'stored', 'securely', 'at', 'one', 'centralized', 'location', 'for', 'subsequent', 'reference', 'and', 'requires', 'strong', 'protections', 'around', 'it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxpcIVYtVkoL",
        "outputId": "1f717f9c-1706-46ea-ecce-ea34125f1b2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing using NLTK word tokenizer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(doc)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu0XsVaCVFaZ",
        "outputId": "2b8d48c6-4d4e-47ac-9672-73e66f49990f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'refers', 'to', 'a', 'process', 'by', 'which', 'a', 'piece', 'of', 'sensitive', 'data', ',', 'such', 'as', 'a', 'credit', 'card', 'number', ',', 'is', 'replaced', 'by', 'a', 'surrogate', 'value', 'known', 'as', 'a', 'token', '.', 'The', 'sensitive', 'data', 'still', 'generally', 'needs', 'to', 'be', 'stored', 'securely', 'at', 'one', 'centralized', 'location', 'for', 'subsequent', 'reference', 'and', 'requires', 'strong', 'protections', 'around', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK's word tokeniser not only breaks on whitespaces but also breaks contraction words such as he'll into \"he\" and \"'ll\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token"
      ],
      "metadata": {
        "id": "2aVotmJJV9JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sentence tokenizer"
      ],
      "metadata": {
        "id": "SDKOkziRWCWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(doc)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA7cibNrVctO",
        "outputId": "285122d9-3158-4a3a-ca17-3cc98c73b2c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token.', 'The sensitive data still generally needs to be stored securely at one centralized location for subsequent reference and requires strong protections around it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tweet tokenizer"
      ],
      "metadata": {
        "id": "5dC6m6jBWdxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A problem with word tokeniser is that it fails to tokeniser emojis and other complex special characters such as word with hashtags.\n",
        "\n",
        "The word tokeniser breaks the emoji '<3' into '<' and '3' which is something that we don't want. Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is."
      ],
      "metadata": {
        "id": "dAibRirQWkp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()"
      ],
      "metadata": {
        "id": "i-gEKstYWL87"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"i recently watched this show called mindhunters:). i totally loved it ðŸ˜. it was gr8 <3. #bingewatching #nothingtodo ðŸ˜Ž\""
      ],
      "metadata": {
        "id": "ttbSWDEfW3Vt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(message))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7jgT3Q6W7j2",
        "outputId": "4ca1c08b-c723-4611-c427-ff9d42a754c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'ðŸ˜', '.', 'it', 'was', 'gr8', '<', '3', '.', '#', 'bingewatching', '#', 'nothingtodo', 'ðŸ˜Ž']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tknzr.tokenize(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-KBE6ZVW-0k",
        "outputId": "e1110c6c-2b03-4064-8467-0f3f39159f6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'recently',\n",
              " 'watched',\n",
              " 'this',\n",
              " 'show',\n",
              " 'called',\n",
              " 'mindhunters',\n",
              " ':)',\n",
              " '.',\n",
              " 'i',\n",
              " 'totally',\n",
              " 'loved',\n",
              " 'it',\n",
              " 'ðŸ˜',\n",
              " '.',\n",
              " 'it',\n",
              " 'was',\n",
              " 'gr8',\n",
              " '<3',\n",
              " '.',\n",
              " '#bingewatching',\n",
              " '#nothingtodo',\n",
              " 'ðŸ˜Ž']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}